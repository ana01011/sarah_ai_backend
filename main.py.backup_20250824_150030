"""
Sarah AI - OPTIMIZED Production Version
Performance-optimized for faster response generation
"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel
from llama_cpp import Llama
import time
import psutil
import re
from typing import Optional, Dict, List
from datetime import datetime
import json
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
import hashlib

# Load environment
load_dotenv()

# ============= PERFORMANCE OPTIMIZATIONS =============

# 1. Response cache for common questions
RESPONSE_CACHE = {}
CACHE_TTL = 3600  # 1 hour

# 2. Memory system
USER_MEMORY = {}

# ============= IMPORT DATABASE AND SERVICES =============
from app.core.database import db
from app.core.config import settings
from app.services.llm_service import llm_service

# Import working routers
from app.api.v1.routers import auth_router
from app.api.v1.routers import chat_router
from app.api.v1.routers import user_router

# Fix auth_service
from app.services.auth.auth_service import auth_service
from jose import jwt, JWTError
from app.models.auth import User

# Add missing methods to auth_service
def decode_token(token: str) -> Optional[dict]:
    try:
        SECRET_KEY = os.getenv("JWT_SECRET", "your-secret-key-change-this")
        ALGORITHM = os.getenv("JWT_ALGORITHM", "HS256")
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None

async def get_user_by_id(user_id: str) -> Optional[User]:
    user = await db.fetchrow("SELECT * FROM users WHERE id = $1", user_id)
    if user:
        return User(**dict(user))
    return None

auth_service.decode_token = decode_token
auth_service.get_user_by_id = get_user_by_id

# ============= OPTIMIZED LLAMA MODEL CONFIGURATION =============
print("🚀 Loading OPTIMIZED Sarah AI Model...")

# Get optimal thread count
cpu_count = psutil.cpu_count(logical=False)  # Physical cores only
optimal_threads = min(cpu_count - 1, 6)  # Leave 1 core for system

# OPTIMIZED MODEL SETTINGS
model = Llama(
    model_path="openhermes-2.5-mistral-7b.Q4_K_M.gguf",
    
    # Context and batch optimization
    n_ctx=1024,  # can be Reduced from 1024 - smaller context = faster
    n_batch=512,  # Increased batch size for better throughput
    
    # CPU optimization
    n_threads=optimal_threads,  # Use optimal thread count
    n_threads_batch=optimal_threads,  # Batch processing threads
    
    # Memory optimization
    use_mmap=True,  # Memory-mapped files
    use_mlock=False,  # Don't lock memory (can cause issues)
    
    # GPU optimization (if available)
    n_gpu_layers=0,  # Set to 20-35 if you have GPU
    
    # Other optimizations
    low_vram=True,  # Low VRAM mode
    f16_kv=True,  # Use 16-bit for key/value cache
    logits_all=False,  # Don't compute logits for all tokens
    vocab_only=False,
    embedding=False,
    
    # Sampling optimization
    rope_freq_base=10000.0,
    rope_freq_scale=1.0,
    
    verbose=False
)

print(f"✅ Model loaded with {optimal_threads} threads on {cpu_count} physical cores")

# ============= WARM UP MODEL (Important for speed) =============
print("🔥 Warming up model...")
try:
    _ = model("Hello", max_tokens=1, temperature=0.1, echo=False)
    print("✅ Model warmed up and ready!")
except Exception as e:
    print(f"⚠️ Warmup failed: {e}")

# ============= LIFESPAN MANAGER =============
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("🚀 Starting Sarah AI OPTIMIZED Server...")
    await db.connect()
    print("✅ Database connected")
    
    try:
        llm_service.load_model()
        print("✅ LLM Service initialized")
    except:
        print("📌 Using local optimized Llama model")
    
    print("✅ Sarah AI Ready for FAST responses!")
    yield
    await db.disconnect()
    print("👋 Sarah AI Server Stopped")

# ============= CREATE FASTAPI APP =============
app = FastAPI(
    title="Sarah AI - Optimized Production API",
    description="Performance-optimized AI Assistant",
    version="2.1.0",
    lifespan=lifespan
)

# ============= CORS MIDDLEWARE =============
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============= REQUEST MODELS =============
class ChatRequest(BaseModel):
    message: str
    max_tokens: int = 100  # Reduced default for speed
    temperature: float = 0.7
    user_id: Optional[str] = "default"
    use_cache: bool = True  # Enable caching by default

# ============= HELPER FUNCTIONS =============
def get_cache_key(message: str, user_id: str = "default") -> str:
    """Generate cache key for responses"""
    return hashlib.md5(f"{message.lower().strip()}:{user_id}".encode()).hexdigest()

def is_identity_question(message):
    msg = message.lower()
    identity_words = [
        'who created', 'who made', 'who built', 'who designed',
        'who developed', 'who are you', 'what are you',
        'created by', 'made by', 'built by', 'designed by',
        'your creator', 'your developer', 'your maker',
        'openai', 'open ai', 'chatgpt', 'gpt', 'anthropic', 'claude'
    ]
    return any(word in msg for word in identity_words)

def clean_response(text):
    replacements = {
        r'\b[Oo]pen\s?AI\b': 'my developers',
        r'\b[Cc]hat\s?GPT\b': 'Sarah AI',
        r'\b[Gg]PT[-\s]?\d*\b': 'Sarah AI',
        r'\b[Aa]nthrop[ic]*\b': 'my developers',
        r'\b[Cc]laude\b': 'Sarah AI',
    }
    for pattern, replacement in replacements.items():
        text = re.sub(pattern, replacement, text)

    problem_words = ['openai', 'open ai', 'chatgpt', 'gpt-', 'anthropic', 'claude']
    if any(word in text.lower() for word in problem_words):
        return "I'm Sarah AI, an independent AI assistant. How can I help you?"

    return text

# ============= OPTIMIZED GENERATION FUNCTION =============
def generate_response_sync(prompt: str, max_tokens: int, temperature: float) -> str:
    """Synchronous response generation with optimizations"""
    try:
        response = model(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_k=40,  # Limit vocabulary
            top_p=0.95,  # Nucleus sampling
            repeat_penalty=1.1,  # Prevent repetition
            stop=["User:", "\n\n", "Human:", "Assistant:"],
            echo=False,
            stream=False  # Disable streaming for speed
        )
        return response['choices'][0]['text'].strip()
    except Exception as e:
        print(f"Generation error: {e}")
        return "I'm having trouble generating a response. Please try again."

# ============= INCLUDE ROUTERS =============
app.include_router(auth_router.router, prefix="/api/v1/auth", tags=["Authentication"])
app.include_router(chat_router.router, prefix="/api/v1/chat", tags=["Chat"])
app.include_router(user_router.router, prefix="/api/v1/users", tags=["Users"])

# ============= OPTIMIZED CHAT ENDPOINTS =============

@app.post("/api/chat")
async def chat(request: ChatRequest):
    """Optimized chat endpoint with caching"""
    start = time.time()
    
    # Check cache first
    if request.use_cache:
        cache_key = get_cache_key(request.message)
        if cache_key in RESPONSE_CACHE:
            cached = RESPONSE_CACHE[cache_key]
            if time.time() - cached['timestamp'] < CACHE_TTL:
                elapsed = time.time() - start
                return {
                    "response": cached['response'],
                    "role": "general",
                    "cached": True,
                    "stats": {
                        "time": round(elapsed, 3),
                        "tokens": len(cached['response'].split()),
                        "tokens_per_second": "cached"
                    }
                }

    # Handle identity questions quickly
    if is_identity_question(request.message):
        response_text = "I'm Sarah AI, an independent AI assistant created by independent developers using open-source technology."
    else:
        # Generate response in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        prompt = f"User: {request.message}\nAssistant:"
        
        response_text = await loop.run_in_executor(
            executor,
            generate_response_sync,
            prompt,
            request.max_tokens,
            request.temperature
        )
        response_text = clean_response(response_text)
        
        # Cache the response
        if request.use_cache:
            cache_key = get_cache_key(request.message)
            RESPONSE_CACHE[cache_key] = {
                'response': response_text,
                'timestamp': time.time()
            }

    elapsed = time.time() - start
    return {
        "response": response_text,
        "role": "general",
        "cached": False,
        "stats": {
            "time": round(elapsed, 3),
            "tokens": len(response_text.split()),
            "tokens_per_second": round(len(response_text.split())/elapsed, 1) if elapsed > 0 else 0
        }
    }

@app.post("/api/chat/with-memory")
async def chat_with_memory(request: ChatRequest):
    """Optimized chat with memory"""
    start = time.time()
    user_id = request.user_id or "default"

    if user_id not in USER_MEMORY:
        USER_MEMORY[user_id] = []

    # Check for identity question
    if is_identity_question(request.message):
        response_text = "I'm Sarah AI, an independent AI assistant created by independent developers."
    else:
        # Build OPTIMIZED context prompt
        prompt = ""
        
        if USER_MEMORY[user_id]:
            # Only use last 2 exchanges for speed
            prompt = "You are Sarah AI. Recent context:\n"
            for exchange in USER_MEMORY[user_id][-2:]:
                prompt += f"U: {exchange['user'][:50]}\n"  # Truncate for speed
                prompt += f"A: {exchange['assistant'][:50]}\n"
            prompt += f"\nUser: {request.message}\nAssistant:"
        else:
            prompt = f"User: {request.message}\nAssistant:"

        # Generate response asynchronously
        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(
            executor,
            generate_response_sync,
            prompt,
            request.max_tokens,
            request.temperature
        )
        response_text = clean_response(response_text)

    # Store in memory
    USER_MEMORY[user_id].append({
        "user": request.message,
        "assistant": response_text,
        "timestamp": datetime.now().isoformat()
    })

    # Keep only last 5 exchanges for speed
    if len(USER_MEMORY[user_id]) > 5:
        USER_MEMORY[user_id] = USER_MEMORY[user_id][-5:]

    elapsed = time.time() - start

    return {
        "response": response_text,
        "user_id": user_id,
        "memory_size": len(USER_MEMORY[user_id]),
        "stats": {
            "time": round(elapsed, 3),
            "context_used": len(USER_MEMORY[user_id]) > 1,
            "tokens_per_second": round(len(response_text.split())/elapsed, 1) if elapsed > 0 else 0
        }
    }

# ============= PERFORMANCE MONITORING ENDPOINT =============
@app.get("/api/performance")
async def performance_stats():
    """Get performance statistics"""
    cpu_percent = psutil.cpu_percent(interval=0.1)
    memory = psutil.virtual_memory()
    
    return {
        "cpu": {
            "percent": cpu_percent,
            "cores": psutil.cpu_count(),
            "physical_cores": psutil.cpu_count(logical=False),
            "threads_used": optimal_threads
        },
        "memory": {
            "percent": memory.percent,
            "available_gb": round(memory.available / (1024**3), 2),
            "total_gb": round(memory.total / (1024**3), 2)
        },
        "cache": {
            "entries": len(RESPONSE_CACHE),
            "memory_sessions": len(USER_MEMORY)
        },
        "model": {
            "context_size": 512,
            "batch_size": 512,
            "threads": optimal_threads
        }
    }

# ============= OTHER ENDPOINTS (kept minimal for reference) =============

@app.get("/api/memory/{user_id}")
async def get_user_memory(user_id: str):
    return {
        "user_id": user_id,
        "conversations": USER_MEMORY.get(user_id, []),
        "total": len(USER_MEMORY.get(user_id, []))
    }

@app.delete("/api/memory/{user_id}")
async def clear_user_memory(user_id: str):
    if user_id in USER_MEMORY:
        del USER_MEMORY[user_id]
    return {"message": f"Memory cleared for {user_id}"}

@app.get("/")
async def root():
    return {
        "name": "Sarah AI Optimized API",
        "version": "2.1.0",
        "status": "running",
        "optimization": "enabled",
        "performance": "/api/performance"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "optimized": True,
        "cache_size": len(RESPONSE_CACHE),
        "memory_sessions": len(USER_MEMORY)
    }

# ============= MAIN ENTRY POINT =============
if __name__ == "__main__":
    import uvicorn
    
    host = os.getenv("API_HOST", "0.0.0.0")
    port = int(os.getenv("API_PORT", "8000"))
    
    # Use multiple workers for better concurrency
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=False,
        workers=1,  # Keep at 1 since model can't be shared across processes
        loop="uvloop",  # Faster event loop (install with: pip install uvloop)
        access_log=False  # Disable access logs for speed
    )
